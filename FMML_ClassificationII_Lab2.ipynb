{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manikantachowdar/Fmml-labs/blob/main/FMML_ClassificationII_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x9mXnIyH_TU"
      },
      "source": [
        "# Lab 2\n",
        "# Classification II : Introduction to Decision Trees\n",
        "\n",
        "```\n",
        "Module Coordinator : Akshit Garg\n",
        "\n",
        "Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of some property by inferring simple decision rules from the data features.\n",
        "\n",
        "\n",
        "Let us take a look at an example of a decision tree which predicts the class of the species of Iris flower from the iris dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRLg2DwJTqMN"
      },
      "source": [
        "#Importing the necessary packages\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8eLU5YOhiPH"
      },
      "source": [
        "### Code for the core experiment:\n",
        "\n",
        "- Creating the decision tree classifier based on parameters passed.\n",
        "- Evaluating the classifier's accuracy and plotting its confusion matrix.\n",
        "- Plotting its decision boundary.\n",
        "- Creating and showing the visualization of the tree made.\n",
        "\n",
        "**SKIP THE CODE IN THE FOLLOWING CELL FOR NOW AND COME BACK TO IT LATER AFTER UNDERSTANDING THE IDEA AND INTUITION BEHIND DECISION TREES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNo2x3Tbhhu3"
      },
      "source": [
        "def performExperiment(trainSet : tuple, testSet : tuple, max_depth : int = None, feature_names : list = None, class_names : list = None, criterion = \"gini\", min_samples_split : int = 2 , min_samples_leaf = 1):\n",
        "  #Importing the Decision tree classifier from sklearn:\n",
        "\n",
        "  clf = tree.DecisionTreeClassifier(max_depth = max_depth, \\\n",
        "                                    criterion = criterion,\\\n",
        "                                    min_samples_split = min_samples_split,\\\n",
        "                                    min_samples_leaf = min_samples_leaf,\\\n",
        "                                    splitter = \"best\",\\\n",
        "                                    random_state = 0,\\\n",
        "                                    )\n",
        "  X_train, y_train = trainSet\n",
        "  X_test, y_test = testSet\n",
        "\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  print(\"Accuracy of the decision tree on the test set: \\n\\n{:.3f}\\n\\n\".format(accuracy_score(y_pred, y_test)))\n",
        "\n",
        "  print(\"Here is a diagram of the tree created to evaluate each sample:\")\n",
        "  fig, ax = plt.subplots(figsize=(12,10))\n",
        "  imgObj = tree.plot_tree(clf, filled=True, ax=ax, feature_names = feature_names, class_names = class_names, impurity=False, proportion=True, rounded=True, fontsize = 12)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def giveAnExample(n : int):\n",
        "  performExperiment((X_train, y_train),  (X_test, y_test), feature_names = iris[\"feature_names\"], class_names = iris[\"target_names\"], max_depth = n)\n",
        "\n",
        "def plotDecisionBoundary(X, y, pair, clf):\n",
        "  x_min, x_max = X[:, pair[0]].min() - 1, X[:, pair[0]].max() + 1\n",
        "  y_min, y_max = X[:, pair[1]].min() - 1, X[:, pair[1]].max() + 1\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                      np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "  y_pred = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "  y_pred = y_pred.reshape(xx.shape)\n",
        "  plt.figure(figsize=(8,6))\n",
        "  plt.contourf(xx, yy, y_pred, alpha=0.4)\n",
        "  plt.scatter(X[:, pair[0]], X[:, pair[1]], c = y, s = 50, edgecolor='k')\n",
        "  plt.title(\"Decision Boundary for two features used in Decision Tree\")\n",
        "  # plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsxUs3oYbFNK"
      },
      "source": [
        "## Loading IRIS Dataset:\n",
        "\n",
        "### About the IRIS dataset:\n",
        "\n",
        "The Iris Dataset contains four features (length and width of sepals and petals) of 50 samples of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). We shall be using decision trees to try to predict the correct species of the flower using these four features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VR4gNQ5Vuwk"
      },
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "irisData = pandas.DataFrame(\\\n",
        "    data = np.hstack((X,y.reshape(y.shape[0], 1), [[iris[\"target_names\"][int(classIdx)]] for classIdx in y])), \\\n",
        "    columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', \"Class\", \"ClassName\"])\n",
        "irisData.sample(n = 10, random_state = 1)\n",
        "\n",
        "#Here is a few samples: The dataset has 4 non-catagorical features and a class which can take of one of the three values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oN3nv1ExEcJ"
      },
      "source": [
        "## Example of DT on Iris dataset with performace evaluation, and tree structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxwk-zEuqc9I"
      },
      "source": [
        "giveAnExample(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkrm75u1x9RX"
      },
      "source": [
        "### Exercise 1:\n",
        " Kindly use the above tree to evaluate the classes for the following examples and verify what percent of them are classified correctly by the tree:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k31oDgYlw2cg"
      },
      "source": [
        "irisData.sample(n = 5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjTv9DSA1zbg"
      },
      "source": [
        "Now let us see how we perform when we try to have a more complex decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKKqh4_BzB4K"
      },
      "source": [
        "giveAnExample(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS7wSLX5zTIr"
      },
      "source": [
        "### Exercise 2:\n",
        "Repeat Exercise 1 for the above tree as well.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "We observe that even though that the tree had four features available to it, the tree uses only two of them to classify the cases of species. It gives us an idea that those two features chosen are performing quite decently. Let us examine the decision boundary generated by the tree when only those two features namely **petal length and petal width** are used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJN66PO-4Q0h"
      },
      "source": [
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming plotDecisionBoundary is a function defined elsewhere\n",
        "def plotDecisionBoundary(X, y, feature_indices, classifier):\n",
        "    # Your implementation for plotting the decision boundary goes here\n",
        "    # Example code (modify based on your implementation):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k', s=20)\n",
        "\n",
        "    # Create a meshgrid for decision boundary visualization\n",
        "    h = .02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Predict the labels for each point in the meshgrid\n",
        "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    plt.contourf(xx, yy, Z, cmap='viridis', alpha=0.3)\n",
        "\n",
        "    plt.xlabel('Petal Length')\n",
        "    plt.ylabel('Petal Width')\n",
        "    plt.title('Decision Boundary of the Decision Tree Classifier')\n",
        "    plt.show()\n",
        "\n",
        "# Assuming X and y are properly defined\n",
        "\n",
        "# Define pair as the indices of 'petal length' and 'petal width'\n",
        "pair = [2, 3]\n",
        "\n",
        "# Create a DecisionTreeClassifier\n",
        "clf = tree.DecisionTreeClassifier(random_state=0, max_depth=3)\n",
        "\n",
        "# Check if pair indices are valid\n",
        "if max(pair) >= X.shape[1]:\n",
        "    print(\"Error: Invalid column index in pair.\")\n",
        "else:\n",
        "    # Fit the classifier\n",
        "    clf.fit(X[:, pair], y)\n",
        "\n",
        "    # Visualize the decision boundary using only 'petal length' and 'petal width'\n",
        "    plotDecisionBoundary(X[:, pair], y, pair, clf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQ6VO7J78vp"
      },
      "source": [
        "**Decision boundary** with considering **sepal width and length**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKnuGzgf6eoZ"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state=0, max_depth=3)\n",
        "pair = [0, 1]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fVoKPWL8W-B"
      },
      "source": [
        "**Decision boundary** with considering **sepal length and pedal length**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXSDC6Il7wDA"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "pair = [0, 2]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzKiQd5Y8ox5"
      },
      "source": [
        "**Decision boundary** with considering **sepal width and pedal width**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6StxQVn8i01"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "pair = [1, 3]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uuefe-k_dAO"
      },
      "source": [
        "---\n",
        "\n",
        "### Exercise 3:\n",
        "\n",
        "#### 3.1 :\n",
        "We see that the above decision boundaries are with depth of 3. Compare the above boundary with trees that have higher complexity (by changing the value of `max_depth`) and then pause and ponder.\n",
        "\n",
        "Test with `max_depth` of the following values:\n",
        "- 2\n",
        "- 5\n",
        "- 10\n",
        "\n",
        "\n",
        "What do you observe?\n",
        "\n",
        "#### 3.2 :\n",
        "\n",
        "On a closer look, we see that the decision boundaries' lines are always at a right angle to the principle axes. Can you reason on why is that the case? \\\n",
        "`(Hint: How is a decision made at any node?)`\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 4:\n",
        "\n",
        "Complete the following function predict: which takes in four variables : `sepal width, sepal length, petal width, petal length` and returns the class of the flower. Use the decision tree made in Exercise 2 and realise the logic using multiple nested `if else` statements."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#execise3.1\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming plotDecisionBoundary is a function defined elsewhere\n",
        "def plotDecisionBoundary(X, y, feature_indices, classifier):\n",
        "    # Your implementation for plotting the decision boundary goes here\n",
        "    pass\n",
        "\n",
        "# Assuming X and y are properly defined\n",
        "\n",
        "# List of max_depth values to test\n",
        "max_depth_values = [2, 5, 10]\n",
        "\n",
        "# Iterate over different max_depth values\n",
        "for max_depth_value in max_depth_values:\n",
        "    # Create a DecisionTreeClassifier with the current max_depth value\n",
        "    clf = tree.DecisionTreeClassifier(random_state=0, max_depth=max_depth_value)\n",
        "\n",
        "    # Check if feature indices are valid (modify based on your actual feature indices)\n",
        "    pair = [0, 1]\n",
        "    if max(pair) >= X.shape[1]:\n",
        "        print(\"Error: Invalid column index in pair.\")\n",
        "    else:\n",
        "        # Fit the classifier\n",
        "        clf.fit(X[:, pair], y)\n",
        "\n",
        "        # Visualize the decision boundary\n",
        "        plt.figure()\n",
        "        plotDecisionBoundary(X[:, pair], y, pair, clf)\n",
        "        plt.title(f'Decision Boundary (max_depth={max_depth_value})')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "biM8TJ-ZVCcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**exercise3.2**\n",
        "\n",
        "The decision boundaries being aligned with the axes in a Decision Tree are a direct consequence of how the tree makes decisions at each node. In a Decision Tree, each internal node represents a decision based on the value of a specific feature. The decision is typically a binary one, splitting the data into two branches based on a threshold value for that feature.\n",
        "\n",
        "The reason the decision boundaries are perpendicular to the axes is because each split is along a single feature at a time. Consider a 2D space with two features (x and y). At each internal node, the tree decides to split the data based on the value of one feature. This creates a vertical or horizontal decision boundary, which is perpendicular to the axis of the chosen feature.\n",
        "\n",
        "For example, if the tree decides to split based on the x-axis, the decision boundary will be a vertical line, and if it decides to split based on the y-axis, the decision boundary will be a horizontal line. This process continues recursively at each subsequent node in the tree.\n",
        "\n",
        "In a more general sense, the axis-aligned decision boundaries are a consequence of the tree's hierarchical, axis-parallel splitting strategy. While this strategy might be effective in certain scenarios, it has limitations in capturing more complex relationships in the data, which is one of the reasons why more advanced techniques like ensemble methods (e.g., Random Forests) or non-linear models may be used when the relationships are more intricate.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lXnj5rqgVi5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#exercise4\n",
        "def predict(sepal_width, sepal_length, petal_width, petal_length):\n",
        "    # Decision Tree logic based on Exercise 2\n",
        "\n",
        "    if petal_width <= 0.8:\n",
        "        if petal_length <= 4.75:\n",
        "            return \"setosa\"\n",
        "        else:\n",
        "            return \"versicolor\"\n",
        "    else:\n",
        "        if petal_length <= 4.95:\n",
        "            return \"versicolor\"\n",
        "        else:\n",
        "            return \"virginica\"\n",
        "\n",
        "# Example usage:\n",
        "sepal_width = 3.0\n",
        "sepal_length = 5.0\n",
        "petal_width = 1.0\n",
        "petal_length = 1.5\n",
        "\n",
        "predicted_class = predict(sepal_width, sepal_length, petal_width, petal_length)\n",
        "print(f\"The predicted class is: {predicted_class}\")\n"
      ],
      "metadata": {
        "id": "uMlb2-x8Vz3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAIeeEFY84oA"
      },
      "source": [
        "def predictSpecies(sepal_width, sepal_length, petal_width,  petal_length) -> str :\n",
        "  \"\"\"\n",
        "    Write your program here to return the species of the plant (string) using if else statements.\n",
        "  \"\"\"\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4agDMEMCNeB-"
      },
      "source": [
        "# Entropy and Information:\n",
        "\n",
        "## How are decision trees built?\n",
        "\n",
        "A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous).\n",
        "We use entropy to calculate the homogeneity of a sample.\n",
        "\n",
        "Entropy itself is defined in the following way:\n",
        "\n",
        "$$E(s) = \\sum_{i=1}^c - p_i * log_2(p_i)$$\n",
        "\n",
        "Where $i$ iterates through the classes of the current group and $p_i$ is the probability of choosing an item from class $i$ when a datapoint is randomly picked from the group.\n",
        "\n",
        "At anypoint in the process of making the decision tree. All possible methods of dividing the group are considered (across all features and values of separations) and then the division with the most amount of **Information Gain** is used to divide the current group into two. This is done recursively to finally attain a tree.\n",
        "\n",
        "Here Information Gain is defined by the difference in Entropy of the group before the division and the weighted sum of the entropy of the two groups after division.\n",
        "\n",
        "$$IG(X) = E(s) - E(s, X)$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgnd4qhrM1Km"
      },
      "source": [
        "irisData.sample(n = 10, random_state = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMDVOKZRY1GE"
      },
      "source": [
        "## Exercise 5:\n",
        "Calculate the Entropy of the above collection of 10 datapoints.\n",
        "## Exercise 6:\n",
        "Suggest a decision node (if, else) statement which divides the group into two groups. Also compute the Information Gain in that division step. Compare this with other decision clauses that you can make and intuitively comment on which is better for classification and observe if this has any correlation with the numerical value of Information Gain.\n",
        "\n",
        "---\n",
        "\n",
        "End of Lab 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exercise5\n",
        "import math\n",
        "\n",
        "def calculate_entropy(class_counts):\n",
        "    total_points = sum(class_counts)\n",
        "    entropy = 0.0\n",
        "\n",
        "    for count in class_counts:\n",
        "        if count != 0:\n",
        "            probability = count / total_points\n",
        "            entropy -= probability * math.log2(probability)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "# Example usage:\n",
        "class_counts = [4, 6]  # Example distribution, modify according to your data\n",
        "entropy_value = calculate_entropy(class_counts)\n",
        "print(f\"The entropy of the data points is: {entropy_value}\")\n"
      ],
      "metadata": {
        "id": "grD-tcbAWZxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#exercise6\n",
        "import math\n",
        "\n",
        "def calculate_entropy(class_counts):\n",
        "    total_points = sum(class_counts)\n",
        "    entropy = 0.0\n",
        "\n",
        "    for count in class_counts:\n",
        "        if count != 0:\n",
        "            probability = count / total_points\n",
        "            entropy -= probability * math.log2(probability)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "def calculate_information_gain(entropy_before_split, class_counts_group_1, class_counts_group_2):\n",
        "    total_points_group_1 = sum(class_counts_group_1)\n",
        "    total_points_group_2 = sum(class_counts_group_2)\n",
        "\n",
        "    entropy_after_split = (\n",
        "        (total_points_group_1 / (total_points_group_1 + total_points_group_2)) * calculate_entropy(class_counts_group_1) +\n",
        "        (total_points_group_2 / (total_points_group_1 + total_points_group_2)) * calculate_entropy(class_counts_group_2)\n",
        "    )\n",
        "\n",
        "    information_gain = entropy_before_split - entropy_after_split\n",
        "    return information_gain\n",
        "\n",
        "# Example usage:\n",
        "# Assuming class distribution before split is [5, 5]\n",
        "class_distribution_before_split = [5, 5]\n",
        "\n",
        "# Assuming class distribution for Group 1 and Group 2 after split\n",
        "class_distribution_group_1 = [4, 1]\n",
        "class_distribution_group_2 = [1, 4]\n",
        "\n",
        "# Calculate entropy before split\n",
        "entropy_before_split = calculate_entropy(class_distribution_before_split)\n",
        "\n",
        "# Calculate Information Gain\n",
        "information_gain = calculate_information_gain(entropy_before_split, class_distribution_group_1, class_distribution_group_2)\n",
        "print(f\"Information Gain: {information_gain}\")\n"
      ],
      "metadata": {
        "id": "GXykQ1lJWpgq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}